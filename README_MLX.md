# üçé Legal Assistant + MLX (Apple Silicon)

Utilisation de mod√®les Hugging Face locaux optimis√©s pour MacBook M1/M2/M3.

**‚ö° NOUVEAU : Auto-d√©marrage automatique !**
Le backend d√©marre automatiquement le serveur MLX quand vous s√©lectionnez un mod√®le MLX. Plus besoin de lancer le serveur manuellement !

## D√©marrage rapide (2 minutes)

### 1. Installation

```bash
# Depuis le dossier backend (MLX-LM est install√© par d√©faut)
cd backend
uv sync
```

### 2. D√©marrer Legal Assistant

```bash
# Terminal 1: SurrealDB
surreal start --user root --pass root --bind 0.0.0.0:8002 file:data/surreal.db

# Terminal 2: Backend
cd backend
uv run python main.py

# Terminal 3: Frontend
cd frontend
npm run dev -- -p 3001
```

### 3. S√©lectionner le mod√®le MLX

1. Ouvrir http://localhost:3001
2. Ouvrir un dossier
3. Cliquer sur **"Param√®tres LLM"** dans le chat
4. S√©lectionner `üçé Qwen 2.5 3B (MLX) - Recommand√© Apple Silicon`
5. Envoyer un message

**‚ö° Le serveur MLX d√©marre automatiquement !**
- Au premier message, le mod√®le est t√©l√©charg√© (~2 GB, 1-2 min)
- Un message de statut s'affiche : "üçé D√©marrage du serveur MLX..."
- Puis : "‚úÖ Serveur MLX pr√™t"
- Les messages suivants utilisent le serveur d√©j√† d√©marr√© (instantan√©)

---

## Mod√®les disponibles

| Mod√®le | RAM | Vitesse (M1) | Qualit√© | Meilleur pour |
|--------|-----|--------------|---------|---------------|
| **Qwen 2.5 3B** ‚≠ê | ~2 GB | ~50 tok/s | Excellent | Fran√ßais, l√©ger |
| **Llama 3.2 3B** | ~1.5 GB | ~60 tok/s | Tr√®s bon | Ultra-rapide |
| **Mistral 7B** | ~4 GB | ~35 tok/s | Excellent | Qualit√© max |

---

## Pourquoi MLX ?

**Avantages vs Ollama :**
- ‚úÖ 2x plus rapide sur Apple Silicon
- ‚úÖ RAM r√©duite (~2 GB vs ~4-5 GB)
- ‚úÖ Support complet de function calling
- ‚úÖ Optimis√© Metal (GPU M1/M2/M3)

**Avantages vs Claude :**
- ‚úÖ 100% gratuit
- ‚úÖ 100% local (privacy)
- ‚úÖ Pas de co√ªt API
- ‚úÖ Fonctionne hors ligne

---

## Documentation compl√®te

üìñ **Guide d√©taill√© :** `backend/MLX_GUIDE.md`

**Inclut :**
- Installation pas √† pas
- R√©solution de probl√®mes
- Comparaison de performances
- Exemples de code Python
- Configuration avanc√©e

---

## Liens utiles

- **MLX-LM Documentation:** https://github.com/ml-explore/mlx-examples/tree/main/llms
- **Hugging Face MLX Community:** https://huggingface.co/mlx-community
- **Agno Framework:** https://docs.agno.com
- **Article Medium:** [Running Local HF Models with MLX-LM](https://medium.com/@levchevajoana/running-local-hugging-face-models-with-mlx-lm-and-the-agno-agentic-framework-de134259d34d)

---

## D√©pannage rapide

**Erreur "Connection refused"**
```bash
# V√©rifier si le serveur tourne
lsof -i :8080

# D√©marrer le serveur
mlx_lm.server --model mlx-community/Qwen2.5-3B-Instruct-4bit --port 8080
```

**Serveur crash "Killed: 9"**
```bash
# Utiliser un mod√®le plus l√©ger
mlx_lm.server --model mlx-community/Llama-3.2-3B-Instruct-4bit --port 8080
```

**Plus d'aide :** Voir `backend/MLX_GUIDE.md`

---

**Bon usage de Legal Assistant avec MLX ! üöÄ**
