# Legal Assistant - Documentation de d√©veloppement

> **Note:** Historique d√©taill√© des sessions de d√©veloppement archiv√© dans `docs/archive/SESSIONS_2025-12.md`

---

## üéâ Nouveaut√© : Support MLX (Apple Silicon)

**3 mod√®les Hugging Face locaux optimis√©s M1/M2/M3 :**
- ‚≠ê Qwen 2.5 3B (4-bit) - Recommand√© pour fran√ßais
- Llama 3.2 3B (4-bit) - Ultra-rapide
- Mistral 7B (4-bit) - Meilleure qualit√©

**Avantages :**
- 2x plus rapide qu'Ollama sur Apple Silicon (~50-60 tok/s)
- RAM r√©duite (~2 GB pour Qwen 2.5 3B)
- Support complet de function calling
- Auto-d√©marrage : Le backend d√©marre automatiquement le serveur MLX
- 100% gratuit et local

**Installation :** `uv sync` (install√© par d√©faut)
**Guides complets :** `backend/MLX_GUIDE.md` et `backend/MLX_AUTO_START.md`

---

## √âtat actuel du projet

### Fonctionnalit√©s impl√©ment√©es

1. **Gestion des dossiers**
   - CRUD complet via API REST
   - Types : civil, p√©nal, administratif, familial, commercial, travail, constitutionnel
   - Suppression en cascade : documents, conversations, chunks d'embeddings

2. **Gestion des documents**
   - Upload de fichiers (PDF, Word, images, audio)
   - DataTable avec filtres (nom, type) et tri
   - Fichiers d√©riv√©s automatiquement li√©s (transcription, extraction PDF, TTS)
   - Actions contextuelles selon le type de fichier

3. **Transcription audio**
   - Whisper MLX (mod√®le large-v3-turbo recommand√©)
   - Workflow hybride : Whisper ‚Üí Agent LLM (formatage) ‚Üí Sauvegarde
   - Cr√©ation automatique de fichiers markdown

4. **Agent conversationnel**
   - Chat avec streaming SSE
   - Support multi-providers : Claude (Anthropic), Ollama, MLX
   - **Recherche s√©mantique int√©gr√©e** : utilise automatiquement `semantic_search`
   - M√©moire de conversation dans SurrealDB

5. **Indexation vectorielle et RAG**
   - Embeddings BGE-M3 via sentence-transformers
   - Acc√©l√©ration GPU : MPS (Apple Silicon) / CUDA / CPU
   - Chunking intelligent (400 mots, 50 mots overlap)
   - Recherche s√©mantique dans les documents
   - **Fix critique appliqu√©** : Utilisation de `type::thing()` pour g√©rer les UUIDs SurrealDB

6. **Synth√®se vocale (TTS)**
   - Service edge-tts (Microsoft Edge TTS)
   - 15 voix : 13 fran√ßaises + 2 anglaises
   - G√©n√©ration audio MP3 depuis documents markdown
   - Configuration des voix par d√©faut dans Settings

### Architecture technique

Voir **`ARCHITECTURE.md`** pour la documentation compl√®te :
- Structure des dossiers
- Services backend (SurrealDB, Whisper, Model Factory)
- Patterns Agno (Workflow d√©claratif, hybride, avec classe)
- Routes API
- Composants frontend

### Fichiers cl√©s

| Fichier | Description |
|---------|-------------|
| `backend/auth/helpers.py` | **NOUVEAU** - Helpers d'authentification centralis√©s |
| `backend/utils/id_utils.py` | **NOUVEAU** - Utilitaires pour normalisation des IDs |
| `frontend/src/components/cases/documents-data-table.tsx` | DataTable avec filtres et actions |
| `backend/tools/semantic_search_tool.py` | Outil de recherche s√©mantique (fix `type::thing()` appliqu√©) |
| `backend/routes/chat.py` | Agent conversationnel avec r√®gle de citation des sources |

---

## Guide de s√©lection du mod√®le LLM

### üéØ R√®gle d'or

- **Documents du dossier n√©cessaires ?** ‚Üí **Claude Sonnet 4.5**
- **Conversation simple sans documents (Mac) ?** ‚Üí **MLX Qwen 2.5 3B** ‚≠ê (plus rapide)
- **Conversation simple sans documents (autre) ?** ‚Üí **Ollama Qwen 2.5 7B**

### Claude Sonnet 4.5 - ‚úÖ RECOMMAND√â POUR RAG

**Utiliser pour :**
- Questions n√©cessitant l'acc√®s aux documents
- Recherche s√©mantique ("R√©sume l'arr√™t X", "Qu'est-ce que...")
- Analyse juridique approfondie
- Citation de sources pr√©cises

**Avantages :**
- Support natif de function calling ‚Üí utilise correctement `semantic_search`
- Comprend les instructions de citation des sources
- Raisonnement juridique de haute qualit√©
- Ne hallucine pas

**Inconv√©nients :**
- Co√ªt par token (API Anthropic)
- N√©cessite une connexion Internet

### Ollama Qwen 2.5 7B - ‚ö†Ô∏è CONVERSATIONS SIMPLES UNIQUEMENT

**Utiliser pour :**
- Conversations g√©n√©rales ("Bonjour", "Merci")
- Questions sur l'assistant
- Clarifications

**Avantages :**
- Gratuit (mod√®le local)
- Rapide, fonctionne hors ligne

**Inconv√©nients :**
- ‚ùå **NE SUPPORTE PAS function calling correctement**
- ‚ùå **Hallucine du contenu** si on lui demande de r√©sumer des documents
- ‚ùå Ne cite pas les sources

### MLX Qwen 2.5 3B - ‚≠ê NOUVEAU

**Utiliser pour :**
- Conversations g√©n√©rales sur Apple Silicon (M1/M2/M3)
- D√©veloppement et tests rapides
- Alternative plus rapide qu'Ollama sur Mac

**Avantages :**
- Gratuit, tr√®s rapide (~50-60 tok/s, 2x plus rapide qu'Ollama)
- Excellent en fran√ßais
- Support complet de function calling
- RAM r√©duite (~2 GB)
- Auto-d√©marrage par le backend

**Inconv√©nients :**
- ‚ùå Apple Silicon uniquement (pas Intel)
- ‚ö†Ô∏è Qualit√© l√©g√®rement inf√©rieure √† Claude pour RAG

**üí° En cas de doute :** Choisissez Claude Sonnet 4.5 pour garantir l'acc√®s aux documents.

---

## Prochaines √©tapes sugg√©r√©es

### Imm√©diat

1. **Tester le RAG complet** ‚úÖ PRIORIT√â
   - V√©rifier que l'agent utilise `semantic_search`
   - Mesurer la qualit√© des r√©ponses

2. **Ajuster param√®tres RAG**
   - `top_k` : Actuellement 5, consid√©rer 7-10
   - `min_similarity` : Actuellement 0.5 (50%)
   - `chunk_size` : Actuellement 400 mots
   - `chunk_overlap` : Actuellement 50 mots

### Court terme

1. **Am√©liorer l'agent chat**
   - ‚úÖ FAIT : Recherche s√©mantique int√©gr√©e
   - ‚úÖ FAIT : M√©moire de conversation
   - ‚ùå REPORTER : Extraction d'entit√©s juridiques

2. **UI/UX**
   - ‚ùå REPORTER : Progression de transcription en temps r√©el
   - ‚úÖ FAIT : Pr√©visualisation markdown
   - ‚úÖ FAIT : Historique des conversations (API pr√™te)

### Moyen terme

1. **RAG** ‚úÖ FAIT
   - Indexation avec embeddings BGE-M3
   - Recherche s√©mantique fonctionnelle

2. **Multi-agents avec DuckDuckGo** üí° √Ä EXPLORER
   - Workflow multi-agents pour documentation automatique
   - Utiliser `agno.tools.duckduckgo`

3. **Int√©grations externes** üí° BONNE ID√âE
   - MCP Server pour CanLII (jurisprudence canadienne)
   - MCP Server pour L√©gis Qu√©bec / LegisInfo

### Refactoring identifi√© (2025-12-05)

**Phase 1 - Quick wins :**
- ‚úÖ FAIT : Supprimer scripts racine morts (`debug_surreal.py`, `fix_malformed_doc.py`)
- ‚úÖ FAIT : Extraire auth helpers dans `backend/auth/helpers.py`
- ‚úÖ FAIT : Cr√©er utilitaire ID normalization dans `backend/utils/id_utils.py`

**Phase 2 - Refactoring majeur :**
- ‚ùå √Ä FAIRE : Diviser `documents.py` (2073 lignes) en 3-4 fichiers th√©matiques
  - `documents.py` : CRUD de base + TTS
  - `transcription.py` : Transcription audio + YouTube
  - `extraction.py` : Extraction PDF/texte

**Phase 3 - Documentation :**
- ‚úÖ FAIT : Simplifier CLAUDE.md (archiv√© sessions dans `docs/archive/SESSIONS_2025-12.md`)

---

## D√©marrage rapide

```bash
# Terminal 1: SurrealDB
surreal start --user root --pass root --bind 0.0.0.0:8002 file:data/surreal.db

# Terminal 2: Backend (d√©marre auto le serveur MLX si configur√©)
cd backend
uv run python main.py

# Terminal 3: Frontend
cd frontend
npm run dev -- -p 3001
```

## Notes techniques

**Ports :**
- SurrealDB : 8002
- Backend : 8000
- Frontend : 3001
- MLX Server : 8080 (OpenAI-compatible API)

**Installation :**
- `uv sync` installe toutes les d√©pendances par d√©faut :
  - Whisper (mlx-whisper pour transcription audio)
  - Embeddings (sentence-transformers avec GPU: MPS/CUDA/CPU)
  - TTS (edge-tts pour synth√®se vocale)
  - Docling (extraction avanc√©e PDF avec OCR)
  - MLX-LM (mod√®les HuggingFace via MLX)

**Configuration embeddings :**
```python
# backend/services/document_indexing_service.py
embedding_provider = "local"           # local, ollama, ou openai
embedding_model = "BAAI/bge-m3"       # Mod√®le HuggingFace
chunk_size = 400                       # Mots par chunk
chunk_overlap = 50                     # Mots d'overlap
```

**Configuration TTS :**
```python
# backend/services/tts_service.py
DEFAULT_VOICES = {
    "fr": "fr-FR-DeniseNeural",  # Voix f√©minine fran√ßaise
    "en": "en-CA-ClaraNeural",   # Voix f√©minine anglaise (Canada)
}
# 15 voix disponibles au total
```

**Configuration MLX :**
```python
# backend/config/models.py
# Top 3 mod√®les recommand√©s pour M1 Pro 16 GB
"mlx-community/Qwen2.5-3B-Instruct-4bit"      # ~2 GB RAM, ~50 tok/s
"mlx-community/Llama-3.2-3B-Instruct-4bit"    # ~1.5 GB RAM, ~60 tok/s
"mlx-community/Mistral-7B-Instruct-v0.3-4bit" # ~4 GB RAM, ~35 tok/s
```

**Logs √† surveiller :**
```
# Embeddings
MPS (Metal Performance Shaders) detecte - utilisation du GPU Apple Silicon
Modele BAAI/bge-m3 charge sur mps

# TTS
Service TTS initialis√© avec edge-tts
Audio g√©n√©r√© avec succ√®s: /path/to/file.mp3

# MLX (si configur√©)
MLX server started on http://localhost:8080
```

**Variables d'environnement :**
- Voir `.env.example` ou `ARCHITECTURE.md` pour la configuration compl√®te

---

## Conventions

- Backend en Python avec FastAPI et Agno
- Frontend en TypeScript avec Next.js 14 (App Router) et shadcn/ui
- Base de donn√©es SurrealDB
- Documentation en fran√ßais
- Commits avec message en anglais + footer Claude Code

---

## Ressources

- **Architecture compl√®te** : `ARCHITECTURE.md`
- **Guide MLX** : `backend/MLX_GUIDE.md` et `backend/MLX_AUTO_START.md`
- **Historique sessions** : `docs/archive/SESSIONS_2025-12.md`
