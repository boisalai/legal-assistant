# Guide des ModÃ¨les Locaux - MLX et vLLM

Ce guide explique comment utiliser les modÃ¨les HuggingFace localement avec **auto-dÃ©marrage automatique** des serveurs.

## ğŸ¯ Vue d'ensemble

L'application supporte 3 mÃ©thodes pour charger des modÃ¨les HuggingFace localement :

| Provider | ModÃ¨les | Device | Auto-dÃ©marrage | RecommandÃ© pour |
|----------|---------|--------|----------------|-----------------|
| **MLX** | Convertis MLX uniquement | Apple Silicon (MPS) | âœ… Oui | Mac M1/M2/M3 |
| **vLLM** | **N'IMPORTE QUEL** modÃ¨le HF | CUDA / CPU | âœ… Oui | NVIDIA GPU |
| **Ollama** | ModÃ¨les Ollama | CPU / GPU | âš ï¸ Manuel | Tous |

## ğŸ MLX (Apple Silicon) - **RECOMMANDÃ‰ pour Mac**

### Installation

```bash
# DÃ©jÃ  installÃ© avec uv sync
uv sync
```

### ModÃ¨les disponibles

```python
# 5 modÃ¨les prÃ©configurÃ©s (tous en 4-bit quantization)
"mlx:mlx-community/Qwen2.5-3B-Instruct-4bit"        # â­ RecommandÃ© - FranÃ§ais excellent (~2 GB)
"mlx:mlx-community/Llama-3.2-3B-Instruct-4bit"      # Ultra-rapide (~1.5 GB)
"mlx:mlx-community/Mistral-7B-Instruct-v0.3-4bit"   # Meilleure qualitÃ© (~4 GB)
"mlx:mlx-community/Phi-3-mini-4k-instruct-4bit"     # Legacy (~2 GB)
"mlx:mlx-community/Qwen2.5-7B-Instruct-4bit"        # Plus puissant (~4.5 GB)
```

### Utilisation

**ğŸ‰ AUTO-DÃ‰MARRAGE AUTOMATIQUE !**

1. **Dans l'interface** : SÃ©lectionne un modÃ¨le MLX dans Settings
2. **Pose une question** : Le serveur MLX dÃ©marre automatiquement
3. **C'est tout !** Pas besoin de commandes manuelles

**Logs au dÃ©marrage :**

```
ğŸš€ ModÃ¨le MLX dÃ©tectÃ©: mlx:mlx-community/Qwen2.5-3B-Instruct-4bit
â³ DÃ©marrage automatique du serveur MLX...
ğŸš€ DÃ©marrage serveur MLX avec mlx-community/Qwen2.5-3B-Instruct-4bit...
   Port: 8080
   âš ï¸  Premier dÃ©marrage: tÃ©lÃ©chargement du modÃ¨le (~2-4 GB)
â³ Attente du dÃ©marrage du serveur (max 30s)...
âœ… Serveur MLX dÃ©marrÃ© avec succÃ¨s en 12.3s
   URL: http://localhost:8080/v1
âœ… Serveur MLX prÃªt
```

**Au premier lancement :**
- Le modÃ¨le sera tÃ©lÃ©chargÃ© depuis HuggingFace Hub (~2-4 GB selon le modÃ¨le)
- Temps de tÃ©lÃ©chargement : ~5-10 min selon votre connexion
- Les fois suivantes : dÃ©marrage instantanÃ© (modÃ¨le en cache)

### Performance

**Sur MacBook Pro M1 Pro 16 GB :**
- Qwen 2.5 3B : ~50 tokens/sec
- Llama 3.2 3B : ~60 tokens/sec (le plus rapide)
- Mistral 7B : ~35 tokens/sec (meilleure qualitÃ©)

## ğŸ® vLLM (NVIDIA GPU) - **RECOMMANDÃ‰ pour CUDA**

### Installation

```bash
# Sur systÃ¨me avec CUDA
pip install vllm

# Sur Apple Silicon (mode CPU - lent, MLX recommandÃ©)
pip install vllm
```

### ModÃ¨les disponibles

**âœ¨ N'IMPORTE QUEL modÃ¨le HuggingFace !**

```python
# ModÃ¨les prÃ©configurÃ©s (exemples)
"vllm:Qwen/Qwen2.5-3B-Instruct"          # â­ RecommandÃ© - FranÃ§ais excellent (~6 GB)
"vllm:meta-llama/Llama-3.2-3B-Instruct"  # Ultra-rapide (~6 GB)
"vllm:Qwen/Qwen2.5-7B-Instruct"          # Plus puissant (~14 GB)
"vllm:mistralai/Mistral-7B-Instruct-v0.3"# Meilleure qualitÃ© (~14 GB)

# Mais vous pouvez utiliser N'IMPORTE QUEL modÃ¨le HuggingFace !
"vllm:votre/modele-prefere"
```

### Utilisation

**ğŸ‰ AUTO-DÃ‰MARRAGE AUTOMATIQUE !**

1. **Dans l'interface** : SÃ©lectionne un modÃ¨le vLLM dans Settings
2. **Pose une question** : Le serveur vLLM dÃ©marre automatiquement
3. **C'est tout !** Pas besoin de commandes manuelles

**Logs au dÃ©marrage :**

```
ğŸš€ ModÃ¨le vLLM dÃ©tectÃ©: vllm:Qwen/Qwen2.5-3B-Instruct
â³ DÃ©marrage automatique du serveur vLLM...
ğŸš€ DÃ©marrage serveur vLLM avec Qwen/Qwen2.5-3B-Instruct...
   Port: 8001
   Device: cuda
   âš ï¸  Premier dÃ©marrage: tÃ©lÃ©chargement du modÃ¨le (~6-14 GB)
â³ Attente du dÃ©marrage du serveur (max 60s)...
âœ… Serveur vLLM dÃ©marrÃ© avec succÃ¨s en 45.2s
   URL: http://localhost:8001/v1
âœ… Serveur vLLM prÃªt
```

**Au premier lancement :**
- Le modÃ¨le sera tÃ©lÃ©chargÃ© depuis HuggingFace Hub (~6-14 GB selon le modÃ¨le)
- Temps de tÃ©lÃ©chargement : ~10-20 min selon votre connexion
- vLLM prend plus de temps Ã  dÃ©marrer que MLX (~30-60s)
- Les fois suivantes : modÃ¨le en cache

### Performance

**Sur NVIDIA GPU (exemple RTX 3090) :**
- ModÃ¨les 3B : ~80-100 tokens/sec
- ModÃ¨les 7B : ~40-60 tokens/sec

**âš ï¸ Sur Apple Silicon (CPU mode - pas recommandÃ©) :**
- ModÃ¨les 3B : ~5-10 tokens/sec (trÃ¨s lent)
- **Utilisez MLX Ã  la place !**

## ğŸ”„ Gestion automatique des serveurs

### Switch entre modÃ¨les

**Le manager gÃ¨re automatiquement les transitions :**

1. **MLX â†’ MLX (mÃªme modÃ¨le)** : RÃ©utilise le serveur existant
2. **MLX â†’ MLX (modÃ¨le diffÃ©rent)** : RedÃ©marre avec le nouveau modÃ¨le
3. **MLX â†’ vLLM** : ArrÃªte MLX, dÃ©marre vLLM
4. **vLLM â†’ MLX** : ArrÃªte vLLM, dÃ©marre MLX
5. **MLX/vLLM â†’ Ollama/Claude** : ArrÃªte le serveur local (Ã©conomise RAM)

**Logs lors du switch :**

```
ğŸ”„ Changement de modÃ¨le: mlx-community/Qwen2.5-3B-Instruct-4bit â†’ mlx-community/Mistral-7B-Instruct-v0.3-4bit
ğŸ›‘ ArrÃªt du serveur MLX...
âœ… Serveur MLX arrÃªtÃ©
ğŸš€ DÃ©marrage serveur MLX avec mlx-community/Mistral-7B-Instruct-v0.3-4bit...
```

### ArrÃªt automatique au shutdown

Tous les serveurs sont arrÃªtÃ©s proprement lors de l'arrÃªt de l'application :

```
Legal Assistant API - Shutting down...
ğŸ›‘ ArrÃªt de tous les serveurs de modÃ¨les...
ğŸ›‘ ArrÃªt du serveur MLX (modÃ¨le: mlx-community/Qwen2.5-3B-Instruct-4bit)...
âœ… Serveur MLX arrÃªtÃ©
âœ… Tous les serveurs arrÃªtÃ©s
All model servers stopped
```

## ğŸ“Š API de gestion des serveurs

### VÃ©rifier le statut

```bash
curl http://localhost:8000/api/model-servers/status
```

**RÃ©ponse :**

```json
{
  "mlx": {
    "running": true,
    "model": "mlx-community/Qwen2.5-3B-Instruct-4bit",
    "port": 8080,
    "host": "localhost",
    "url": "http://localhost:8080/v1"
  },
  "vllm": {
    "running": false,
    "model": null,
    "port": 8001,
    "host": "localhost",
    "url": null
  }
}
```

### ArrÃªter tous les serveurs manuellement

```bash
curl -X POST http://localhost:8000/api/model-servers/stop-all
```

**UtilitÃ© :** LibÃ©rer la RAM sans redÃ©marrer l'application.

## âš™ï¸ Configuration

### Ports par dÃ©faut

- **MLX** : `http://localhost:8080/v1`
- **vLLM** : `http://localhost:8001/v1`
- **Backend FastAPI** : `http://localhost:8000` (API REST)

### Variables d'environnement (optionnel)

```bash
# Ports personnalisÃ©s (non implÃ©mentÃ© actuellement)
MLX_SERVER_PORT=8080
VLLM_SERVER_PORT=8001
```

## ğŸ†š Comparaison MLX vs vLLM

| CritÃ¨re | MLX | vLLM |
|---------|-----|------|
| **CompatibilitÃ©** | ModÃ¨les convertis MLX | **Tous les modÃ¨les HF** |
| **Device** | Apple Silicon (MPS) | CUDA / CPU |
| **Vitesse** | âš¡âš¡âš¡ TrÃ¨s rapide | âš¡âš¡ Rapide (CUDA) / âš¡ Lent (CPU) |
| **RAM** | âœ… RÃ©duite (4-bit) | âŒ Plus Ã©levÃ©e (full precision) |
| **DÃ©marrage** | âœ… Rapide (~10-20s) | âš ï¸ Lent (~30-60s) |
| **Installation** | âœ… Inclus (uv sync) | âš ï¸ Manuelle (pip install vllm) |
| **ModÃ¨les** | ~100 modÃ¨les convertis | **Tous les modÃ¨les HF** |

## ğŸ’¡ Recommandations

### Pour Apple Silicon (M1/M2/M3)

âœ… **Utilisez MLX**
- Plus rapide
- Moins de RAM
- Quantization 4-bit
- Installation simple

### Pour NVIDIA GPU

âœ… **Utilisez vLLM**
- Support de tous les modÃ¨les HF
- Optimisations CUDA avancÃ©es
- Pas de conversion nÃ©cessaire

### Pour CPU uniquement

âœ… **Utilisez Ollama**
- Meilleure compatibilitÃ© CPU
- vLLM/MLX sont trop lents sur CPU

## ğŸ› DÃ©pannage

### Erreur : "vLLM n'est pas installÃ©"

```bash
pip install vllm
```

### Erreur : "mlx-lm n'est pas installÃ©"

```bash
uv sync
```

### Le serveur ne dÃ©marre pas

1. VÃ©rifiez les logs pour l'erreur exacte
2. VÃ©rifiez que le port n'est pas dÃ©jÃ  utilisÃ© :
   ```bash
   lsof -i :8080  # MLX
   lsof -i :8001  # vLLM
   ```
3. Essayez d'arrÃªter tous les serveurs :
   ```bash
   curl -X POST http://localhost:8000/api/model-servers/stop-all
   ```

### Timeout au dÃ©marrage

**Causes possibles :**
- Premier tÃ©lÃ©chargement du modÃ¨le (peut prendre 10-20 min)
- Connexion Internet lente
- RAM insuffisante

**Solutions :**
- Attendre le tÃ©lÃ©chargement complet
- Choisir un modÃ¨le plus petit (3B au lieu de 7B)
- VÃ©rifier les logs pour voir la progression

## ğŸ“ Exemples d'utilisation

### Exemple 1 : Utiliser MLX sur Mac

1. SÃ©lectionne `mlx:mlx-community/Qwen2.5-3B-Instruct-4bit` dans Settings
2. Pose ta question dans le chat
3. Le serveur dÃ©marre automatiquement (premiÃ¨re fois : ~10s + tÃ©lÃ©chargement)
4. Les fois suivantes : dÃ©marrage instantanÃ©

### Exemple 2 : Tester plusieurs modÃ¨les MLX

1. Essaye d'abord Qwen 2.5 3B (franÃ§ais excellent)
2. Si trop lent, passe Ã  Llama 3.2 3B (plus rapide)
3. Si besoin de qualitÃ©, passe Ã  Mistral 7B (meilleur raisonnement)
4. Le serveur redÃ©marre automatiquement Ã  chaque changement

### Exemple 3 : Utiliser vLLM avec un modÃ¨le custom

1. Trouve un modÃ¨le HuggingFace (ex: `unsloth/Llama-3.2-1B-Instruct`)
2. Ajoute-le dans `backend/config/models.py` :
   ```python
   "unsloth/Llama-3.2-1B-Instruct": {
       "name": "Llama 3.2 1B Instruct",
       "params": "1B",
       "ram": "~2 GB",
       ...
   }
   ```
3. RedÃ©marre le backend
4. SÃ©lectionne `vllm:unsloth/Llama-3.2-1B-Instruct`
5. Le serveur tÃ©lÃ©charge et dÃ©marre automatiquement

## ğŸ“ Conclusion

**L'auto-dÃ©marrage automatique rend l'utilisation de modÃ¨les locaux aussi simple que les API cloud !**

- âœ… Pas besoin de lancer manuellement les serveurs
- âœ… Switch entre modÃ¨les en un clic
- âœ… Gestion automatique de la RAM
- âœ… Toujours via Agno (jamais de LLM direct)

**Profitez-en !** ğŸš€
