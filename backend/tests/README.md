# Tests automatisÃ©s - Legal Assistant

Ce rÃ©pertoire contient les tests automatisÃ©s pour l'application Legal Assistant.

## ğŸ‰ Tests fonctionnels avec serveur rÃ©el!

Les tests d'intÃ©gration utilisent maintenant un **serveur FastAPI rÃ©el** dÃ©marrÃ© automatiquement sur le port 8001. Cette approche garantit des tests rÃ©alistes sans conflits d'event loop.

## âœ… Ã‰tat actuel (2025-12-20)

- **62 tests passent** (100% des tests non-skipped) âœ…
- **4 tests skipped** (tests ML avec donnÃ©es rÃ©elles)
- **99 secondes** d'exÃ©cution
- **14% de couverture** de code (API endpoints)

**Session actuelle** : Correction des bugs de validation dans l'endpoint `/transcribe`
**DÃ©tails complets** : Voir [`IMPLEMENTATION_SUMMARY.md`](./IMPLEMENTATION_SUMMARY.md)

## Installation des dÃ©pendances de test

```bash
# Installer les dÃ©pendances de dÃ©veloppement
uv sync --extra dev
```

## PrÃ©requis

**SurrealDB doit Ãªtre en cours d'exÃ©cution** sur `localhost:8002` :

```bash
# Depuis la racine du projet
docker-compose up -d
# OU en natif
surreal start --user root --pass root --bind 0.0.0.0:8002 file:backend/data/surrealdb/legal.db
```

## ExÃ©cution des tests

### Tous les tests (rapides uniquement)

```bash
# Depuis le rÃ©pertoire backend
cd backend
uv run pytest -m "not slow"

# Avec couverture de code
uv run pytest -m "not slow" --cov=. --cov-report=html
```

### Tous les tests (incluant les tests lents)

```bash
uv run pytest
```

### Tests spÃ©cifiques

```bash
# Tests pour les cours uniquement
uv run pytest tests/test_courses.py

# Test spÃ©cifique
uv run pytest tests/test_courses.py::TestCoursesCRUD::test_create_course

# Tests avec sortie dÃ©taillÃ©e
uv run pytest -v

# Tests avec couverture de code
uv run pytest --cov=. --cov-report=html
```

### Options utiles

```bash
# ArrÃªter au premier Ã©chec
uv run pytest -x

# Afficher les print statements
uv run pytest -s

# Tests parallÃ¨les (plus rapide)
uv run pytest -n auto  # NÃ©cessite pytest-xdist

# Tests avec markers
uv run pytest -m unit        # Uniquement les tests unitaires
uv run pytest -m integration # Uniquement les tests d'intÃ©gration
```

## Structure des tests

```
tests/
â”œâ”€â”€ __init__.py                      # Package de tests
â”œâ”€â”€ conftest.py                      # Configuration pytest et fixtures globales
â”œâ”€â”€ test_courses.py                  # Tests CRUD pour les cours (12 tests)
â”œâ”€â”€ test_documents.py                # Tests upload/download de documents (11 tests)
â”œâ”€â”€ test_chat.py                     # Tests chat et streaming SSE (13 tests)
â”œâ”€â”€ test_semantic_search.py          # Tests recherche sÃ©mantique et RAG (9 tests)
â”œâ”€â”€ test_linked_directories.py       # Tests liaison de rÃ©pertoires (11 tests)
â”œâ”€â”€ test_transcription.py            # Tests transcription audio (11 tests)
â”œâ”€â”€ KNOWN_ISSUES.md                  # Documentation technique et solutions
â””â”€â”€ README.md                        # Ce fichier
```

**Total : 66 tests** (55 rapides, 11 marquÃ©s comme `slow`)

## Ã‰criture de nouveaux tests

### Structure d'un test

```python
import pytest
from httpx import AsyncClient

@pytest.mark.asyncio
async def test_example(client: AsyncClient):
    """Description du test."""
    # Arrange - PrÃ©parer les donnÃ©es
    data = {"title": "Test"}

    # Act - ExÃ©cuter l'action
    response = await client.post("/api/courses", json=data)

    # Assert - VÃ©rifier les rÃ©sultats
    assert response.status_code == 201
    assert response.json()["title"] == "Test"
```

### Fixtures disponibles (conftest.py)

#### Fixtures de session (partagÃ©es entre tous les tests)

- `test_server`: Serveur FastAPI rÃ©el sur http://localhost:8001
- `auth_token`: Token JWT pour l'authentification
- `event_loop`: Event loop partagÃ© pour Ã©viter les problÃ¨mes de fermeture

#### Fixtures par fonction (nouvelles pour chaque test)

- `client`: Client HTTP asynchrone avec authentification
  - Base URL: `http://localhost:8001`
  - Headers: `Authorization: Bearer <token>`
  - Timeout: 60 secondes

#### Fixtures spÃ©cifiques par fichier de test

Chaque fichier de test dÃ©finit ses propres fixtures pour crÃ©er des donnÃ©es de test (cours, documents, etc.).

## Couverture de code

AprÃ¨s avoir exÃ©cutÃ© les tests avec `--cov`, ouvrez le rapport HTML :

```bash
# GÃ©nÃ©rer le rapport
uv run pytest --cov=. --cov-report=html

# Ouvrir le rapport (macOS)
open htmlcov/index.html

# Linux
xdg-open htmlcov/index.html
```

## Bonnes pratiques

1. **Isolation** : Chaque test doit Ãªtre indÃ©pendant
2. **Nettoyage** : Utiliser les fixtures pour nettoyer aprÃ¨s les tests
3. **Nommage** : Noms descriptifs (`test_create_course_with_valid_data`)
4. **Arrange-Act-Assert** : Structure claire en 3 parties
5. **Tests asynchrones** : Utiliser `@pytest.mark.asyncio` pour les tests async

## CI/CD

Les tests peuvent Ãªtre intÃ©grÃ©s dans un pipeline CI/CD :

```yaml
# Exemple GitHub Actions
- name: Run tests
  run: |
    cd backend
    uv sync --extra dev
    uv run pytest --cov=. --cov-report=xml
```

## DÃ©pannage

### Erreur de connexion Ã  SurrealDB

```
ERROR: Connection refused
```

**Solution** : VÃ©rifiez que SurrealDB est en cours d'exÃ©cution.

### Tests qui Ã©chouent de maniÃ¨re alÃ©atoire

**Cause possible** : Tests non isolÃ©s, donnÃ©es partagÃ©es

**Solution** : VÃ©rifiez que `clean_test_data` fonctionne correctement et que chaque test crÃ©e ses propres donnÃ©es.

### ImportError

```
ImportError: cannot import name 'app' from 'main'
```

**Solution** : Assurez-vous d'Ãªtre dans le rÃ©pertoire `backend` et que l'environnement virtuel est activÃ©.
