# Guide d'utilisation des mod√®les MLX avec Legal Assistant

Ce guide explique comment utiliser les mod√®les Hugging Face localement via MLX-LM avec votre Legal Assistant sur MacBook Pro M1 Pro 16 Go.

## Table des mati√®res

1. [Pr√©requis](#pr√©requis)
2. [Installation](#installation)
3. [Mod√®les recommand√©s](#mod√®les-recommand√©s)
4. [D√©marrage rapide](#d√©marrage-rapide)
5. [Configuration de l'assistant](#configuration-de-lassistant)
6. [R√©solution de probl√®mes](#r√©solution-de-probl√®mes)

---

## Pr√©requis

- **MacBook Pro M1/M2/M3** (Apple Silicon uniquement)
- **16 Go RAM minimum** (recommand√©)
- **Python 3.10+** install√©
- **mlx-lm** install√© (voir installation ci-dessous)

**Note:** MLX est optimis√© exclusivement pour les puces Apple Silicon. Si vous utilisez un Mac Intel, pr√©f√©rez Ollama.

---

## Installation

### 1. Installer MLX-LM

```bash
# Depuis le dossier backend (MLX-LM est install√© par d√©faut)
uv sync

# Ou avec pip (si vous n'utilisez pas uv)
pip install mlx-lm
```

### 2. V√©rifier l'installation

```bash
python3 -c "import mlx_lm; print('‚úÖ MLX-LM install√© avec succ√®s')"
```

---

## Mod√®les recommand√©s

Legal Assistant supporte 3 mod√®les MLX optimis√©s pour votre M1 Pro 16 Go :

| Mod√®le | Taille | RAM | Vitesse | Qualit√© | Meilleur pour |
|--------|--------|-----|---------|---------|---------------|
| **Qwen 2.5 3B (4-bit)** ‚≠ê | 3B | ~2 GB | ~50 tok/s | Excellent | Fran√ßais excellent, l√©ger, rapide |
| **Llama 3.2 3B (4-bit)** | 3B | ~1.5 GB | ~60 tok/s | Tr√®s bon | Ultra-rapide, usage g√©n√©ral |
| **Mistral 7B v0.3 (4-bit)** | 7B | ~4 GB | ~35 tok/s | Excellent | Qualit√© maximale, t√¢ches complexes |

‚≠ê **Recommand√©:** Qwen 2.5 3B est le meilleur choix pour :
- Excellent en fran√ßais (langue principale de Legal Assistant)
- L√©ger et rapide sur M1 Pro
- Support complet du function calling (outils)

---

## D√©marrage rapide

### 1. Lancer le serveur MLX

Le serveur MLX expose une API compatible OpenAI sur le port 8080 :

```bash
# D√©marrer avec Qwen 2.5 3B (recommand√©)
mlx_lm.server --model mlx-community/Qwen2.5-3B-Instruct-4bit --port 8080

# Ou avec Llama 3.2 3B (plus rapide)
mlx_lm.server --model mlx-community/Llama-3.2-3B-Instruct-4bit --port 8080

# Ou avec Mistral 7B (meilleure qualit√©)
mlx_lm.server --model mlx-community/Mistral-7B-Instruct-v0.3-4bit --port 8080
```

**Au premier lancement**, le mod√®le sera t√©l√©charg√© automatiquement depuis Hugging Face (~2-4 GB selon le mod√®le).

### 2. V√©rifier que le serveur fonctionne

```bash
# Test simple
curl http://localhost:8080/v1/models

# Devrait retourner : {"data": [...], "object": "list"}
```

### 3. D√©marrer Legal Assistant

```bash
# Terminal 1: SurrealDB (base de donn√©es)
surreal start --user root --pass root --bind 0.0.0.0:8002 file:data/surreal.db

# Terminal 2: Backend (API)
cd backend
uv run python main.py

# Terminal 3: Frontend (UI)
cd frontend
npm run dev -- -p 3001
```

---

## Configuration de l'assistant

### Via l'interface web (Settings)

1. Ouvrir http://localhost:3001
2. Aller dans **Settings** (param√®tres)
3. Section **Mod√®le IA**
4. S√©lectionner un mod√®le MLX dans le menu d√©roulant :
   - `mlx:mlx-community/Qwen2.5-3B-Instruct-4bit` ‚≠ê
   - `mlx:mlx-community/Llama-3.2-3B-Instruct-4bit`
   - `mlx:mlx-community/Mistral-7B-Instruct-v0.3-4bit`
5. Cliquer sur **Sauvegarder les param√®tres**

**Note:** Les mod√®les MLX ont une ic√¥ne üçé (Apple Silicon) dans le menu.

### Via code Python (pour d√©veloppeurs)

```python
from services.model_factory import create_model
from agno.agent import Agent

# Cr√©er un mod√®le MLX
model = create_model("mlx:mlx-community/Qwen2.5-3B-Instruct-4bit")

# Utiliser dans un agent
agent = Agent(
    name="Legal Assistant",
    model=model,
    instructions="Tu es un assistant juridique expert.",
)

# Tester
agent.print_response("R√©sume l'article 1 du Code civil.")
```

---

## R√©solution de probl√®mes

### Erreur: "Connection refused" ou "API not available"

**Cause:** Le serveur MLX n'est pas d√©marr√©.

**Solution:**
```bash
# V√©rifier si le serveur tourne
lsof -i :8080

# D√©marrer le serveur MLX
mlx_lm.server --model mlx-community/Qwen2.5-3B-Instruct-4bit --port 8080
```

---

### Erreur: "Model not found" ou t√©l√©chargement qui √©choue

**Cause:** Probl√®me de connexion √† Hugging Face ou mod√®le mal nomm√©.

**Solution:**
```bash
# T√©l√©charger manuellement le mod√®le
python3 -c "from huggingface_hub import snapshot_download; snapshot_download('mlx-community/Qwen2.5-3B-Instruct-4bit')"

# Puis relancer le serveur
mlx_lm.server --model mlx-community/Qwen2.5-3B-Instruct-4bit --port 8080
```

---

### Performances lentes (< 10 tokens/sec)

**Causes possibles:**
1. **RAM insuffisante** : Mistral 7B n√©cessite ~4 GB RAM libre. Fermez les applications.
2. **Swap actif** : macOS utilise le swap disque (beaucoup plus lent).
3. **Mod√®le trop lourd** : Passez √† Qwen 2.5 3B ou Llama 3.2 3B.

**Solution:**
```bash
# Utiliser un mod√®le plus l√©ger
mlx_lm.server --model mlx-community/Llama-3.2-3B-Instruct-4bit --port 8080

# Ou fermer les applications gourmandes (Chrome, etc.)
```

---

### Le serveur MLX crash avec "Killed: 9"

**Cause:** M√©moire insuffisante. macOS tue le processus.

**Solution:** Utilisez un mod√®le plus l√©ger ou lib√©rez de la RAM.

```bash
# Mod√®le le plus l√©ger (1.5 GB)
mlx_lm.server --model mlx-community/Llama-3.2-3B-Instruct-4bit --port 8080
```

---

## Avantages de MLX vs Ollama

| Crit√®re | MLX | Ollama |
|---------|-----|--------|
| **Optimisation Apple Silicon** | ‚úÖ Natif | ‚ö†Ô∏è √âmulation |
| **Performance (M1)** | ~50-60 tok/s | ~30-40 tok/s |
| **M√©moire requise** | 1.5-4 GB | 2-5 GB |
| **Installation** | pip install | Application s√©par√©e |
| **Format mod√®les** | HuggingFace (4-bit) | GGUF (quantized) |
| **API** | OpenAI-compatible | OpenAI-compatible |
| **Support GPU** | Metal (MPS) | Metal (MPS) |

**Verdict:** MLX est plus rapide et mieux optimis√© pour Apple Silicon, mais Ollama est plus simple √† installer.

---

## Comparaison avec Claude et Ollama

| Crit√®re | MLX (Local) | Ollama (Local) | Claude (API) |
|---------|-------------|----------------|--------------|
| **Co√ªt** | Gratuit | Gratuit | $3-15 / 1M tokens |
| **Vitesse** | ~50 tok/s | ~30 tok/s | ~70 tok/s (r√©seau) |
| **Qualit√©** | Tr√®s bon | Tr√®s bon | Excellent |
| **Privacy** | 100% local | 100% local | Envoi √† Anthropic |
| **Function calling** | ‚úÖ Oui | ‚úÖ Oui | ‚úÖ Oui (meilleur) |
| **Fran√ßais** | Excellent (Qwen) | Bon | Excellent |
| **RAG / Recherche s√©mantique** | ‚ö†Ô∏è Moyen | ‚ö†Ô∏è Moyen | ‚úÖ Excellent |

**Recommandation selon le cas d'usage:**
- **Questions sur les documents (RAG)** ‚Üí Claude Sonnet 4.5 (meilleure compr√©hension)
- **Conversations simples** ‚Üí MLX Qwen 2.5 3B (rapide, gratuit)
- **D√©veloppement/tests** ‚Üí MLX Llama 3.2 3B (ultra-rapide)

---

## R√©f√©rences

- **MLX-LM Documentation:** https://github.com/ml-explore/mlx-examples/tree/main/llms
- **Hugging Face MLX Community:** https://huggingface.co/mlx-community
- **Agno Framework:** https://docs.agno.com
- **Guide Article (Medium):** [Running Local HF Models with MLX-LM and Agno](https://medium.com/@levchevajoana/running-local-hugging-face-models-with-mlx-lm-and-the-agno-agentic-framework-de134259d34d)

---

## Support

Pour toute question ou probl√®me :
1. V√©rifier cette documentation
2. Consulter les logs du serveur MLX (`mlx_lm.server --model ... --port 8080`)
3. Consulter les logs du backend (`backend/main.py`)
4. Ouvrir une issue sur GitHub

---

**Bon usage de Legal Assistant avec MLX ! üöÄ**
