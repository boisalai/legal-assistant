"""
Factory pour cr√©er des mod√®les Agno selon la configuration.

Ce module fournit des fonctions helpers pour cr√©er des instances
de mod√®les Agno compatibles avec Agent, Team et Workflow.

Patterns officiels Agno:
- Ollama: agno.models.ollama.Ollama (mod√®les locaux)
- Claude: agno.models.anthropic.Claude (API Anthropic)
- MLX: agno.models.openai.OpenAILike (mod√®les HF optimis√©s Apple Silicon)
- vLLM: agno.models.vllm.VLLM (N'IMPORTE QUEL mod√®le HF local CPU/GPU)

Usage:
    from services.model_factory import create_model

    # Ollama (mod√®les locaux cross-platform)
    model = create_model("ollama:mistral")

    # Claude (API Anthropic - meilleure qualit√©)
    model = create_model("anthropic:claude-sonnet-4-5-20250929")

    # vLLM (N'IMPORTE QUEL mod√®le HuggingFace local CPU/GPU)
    model = create_model("vllm:Qwen/Qwen2.5-1.5B-Instruct")  # Ultra-l√©ger pour CPU
    model = create_model("vllm:Qwen/Qwen2.5-3B-Instruct")    # Polyvalent CPU/GPU
    # Auto-d√©marrage du serveur vLLM

    # MLX (mod√®les HF convertis pour Apple Silicon - plus rapide)
    model = create_model("mlx:mlx-community/Qwen2.5-3B-Instruct-4bit")
    # Auto-d√©marrage du serveur MLX

    # Utiliser dans un agent
    agent = Agent(name="Test", model=model)
"""

import logging
import os
from typing import Any, Optional

from config.models import (
    DEFAULT_CLAUDE_MODEL,
    DEFAULT_MLX_MODEL,
    DEFAULT_MLX_SERVER_URL,
    DEFAULT_OLLAMA_MODEL,
)
from config.settings import settings

logger = logging.getLogger(__name__)


def create_model(model_string: str, **kwargs) -> Any:
    """
    Cr√©e une instance de mod√®le Agno √† partir d'une string.

    Format de la string:
        - "ollama:MODEL_NAME" ‚Üí Ollama local
        - "anthropic:MODEL_ID" ‚Üí Claude API
        - "vllm:MODEL_ID" ‚Üí vLLM local (N'IMPORTE QUEL mod√®le HuggingFace CPU/GPU)
        - "mlx:MODEL_PATH" ‚Üí MLX local (mod√®les HuggingFace convertis pour Apple Silicon)
        - "openai:MODEL_ID" ‚Üí OpenAI API

    Args:
        model_string: String de configuration du mod√®le
        **kwargs: Param√®tres additionnels (api_key, base_url, etc.)

    Returns:
        Instance de mod√®le Agno (Ollama, Claude, VLLM, OpenAILike, etc.)

    Examples:
        >>> model = create_model("ollama:mistral")
        >>> model = create_model("anthropic:claude-sonnet-4-5-20250929", api_key="sk-ant-...")
        >>> model = create_model("vllm:Qwen/Qwen2.5-1.5B-Instruct")  # Ultra-l√©ger pour CPU
        >>> model = create_model("mlx:mlx-community/Qwen2.5-3B-Instruct-4bit")  # Apple Silicon

    Raises:
        ValueError: Si le format est invalide ou le provider non support√©
    """
    if ":" not in model_string:
        raise ValueError(
            f"Format invalide: '{model_string}'. "
            f"Format attendu: 'provider:model' (ex: 'ollama:mistral')"
        )

    provider, model_id = model_string.split(":", 1)
    provider = provider.lower().strip()
    model_id = model_id.strip()

    # Compatibilit√© : Rediriger huggingface: vers vllm: (migration)
    if provider == "huggingface":
        logger.warning(f"‚ö†Ô∏è  Provider 'huggingface:' d√©pr√©ci√© - Redirection automatique vers 'vllm:'")
        logger.warning(f"   Ancien: huggingface:{model_id}")
        logger.warning(f"   Nouveau: vllm:{model_id}")
        logger.warning(f"   Mettez √† jour votre configuration pour utiliser 'vllm:' directement")
        provider = "vllm"

    logger.info(f"Creating model: provider={provider}, model={model_id}")

    if provider == "ollama":
        return _create_ollama_model(model_id, **kwargs)
    elif provider == "anthropic":
        return _create_claude_model(model_id, **kwargs)
    elif provider == "mlx":
        return _create_mlx_model(model_id, **kwargs)
    elif provider == "vllm":
        return _create_vllm_model(model_id, **kwargs)
    elif provider == "openai":
        return _create_openai_model(model_id, **kwargs)
    else:
        raise ValueError(
            f"Provider non support√©: '{provider}'. "
            f"Providers support√©s: ollama, anthropic, mlx, vllm, openai"
        )


def _create_ollama_model(model_id: str, **kwargs) -> Any:
    """
    Cr√©e un mod√®le Ollama.

    Args:
        model_id: Nom du mod√®le Ollama (ex: "mistral", "llama3.2")
        **kwargs: Param√®tres additionnels (host, timeout, etc.)

    Returns:
        Instance de agno.models.ollama.Ollama
    """
    try:
        from agno.models.ollama import Ollama
    except ImportError as e:
        raise ImportError(
            "Le package 'ollama' n'est pas install√©. "
            "Installez-le avec: uv sync --extra ollama"
        ) from e

    # Configuration par d√©faut
    host = kwargs.pop("host", None)  # None = utilise la valeur par d√©faut d'Ollama

    if host:
        logger.info(f"‚úÖ Creating Ollama model: {model_id} (host={host})")
    else:
        logger.info(f"‚úÖ Creating Ollama model: {model_id} (default host)")

    return Ollama(
        id=model_id,
        host=host,
        **kwargs
    )


def _create_claude_model(model_id: str, **kwargs) -> Any:
    """
    Cr√©e un mod√®le Claude (Anthropic).

    Args:
        model_id: ID du mod√®le Claude (ex: "claude-sonnet-4-5-20250929")
        **kwargs: Param√®tres additionnels (api_key, etc.)

    Returns:
        Instance de agno.models.anthropic.Claude
    """
    try:
        from agno.models.anthropic import Claude
    except ImportError as e:
        raise ImportError(
            "Le package 'anthropic' n'est pas install√©. "
            "Installez-le avec: uv add anthropic"
        ) from e

    # R√©cup√©rer la cl√© API
    api_key = kwargs.pop("api_key", None) or settings.anthropic_api_key

    if not api_key:
        logger.warning(
            "‚ö†Ô∏è  ANTHROPIC_API_KEY non configur√©e. "
            "Le mod√®le sera cr√©√© mais √©chouera √† l'ex√©cution."
        )

    logger.info(f"‚úÖ Creating Claude model: {model_id}")

    return Claude(
        id=model_id,
        api_key=api_key,
        **kwargs
    )


def _create_mlx_model(model_id: str, **kwargs) -> Any:
    """
    Cr√©e un mod√®le MLX via OpenAI-compatible API.

    Cette m√©thode utilise OpenAILike d'Agno pour se connecter √† un serveur
    MLX qui expose une API compatible OpenAI.

    Setup requis:
        1. Installer mlx-lm: pip install mlx-lm
        2. Lancer le serveur: mlx_lm.server --model MODEL_PATH --port 8080

    Args:
        model_id: Path du mod√®le MLX (ex: "mlx-community/Phi-3-mini-4k-instruct-4bit")
        **kwargs: Param√®tres additionnels (base_url, api_key, etc.)

    Returns:
        Instance de agno.models.openai.OpenAILike configur√©e pour MLX
    """
    try:
        from agno.models.openai import OpenAILike
    except ImportError as e:
        raise ImportError(
            "Le package Agno n'est pas correctement install√©."
        ) from e

    # Configuration par d√©faut
    base_url = kwargs.pop("base_url", DEFAULT_MLX_SERVER_URL)
    api_key = kwargs.pop("api_key", "not-provided")  # MLX server n'a pas besoin de cl√©

    logger.info(f"‚úÖ Creating MLX model via OpenAILike: {model_id}")
    logger.info(f"   Base URL: {base_url}")
    logger.info(f"   Note: Assurez-vous que le serveur MLX est lanc√©!")
    logger.info(f"   Command: mlx_lm.server --model {model_id} --port 8080")

    return OpenAILike(
        id=model_id,
        name=f"MLX {model_id.split('/')[-1]}",
        provider="mlx",
        base_url=base_url,
        api_key=api_key,
        **kwargs
    )


def _create_vllm_model(model_id: str, **kwargs) -> Any:
    """
    Cr√©e un mod√®le vLLM pour charger n'importe quel mod√®le HuggingFace localement.

    vLLM charge les mod√®les HuggingFace localement avec support MPS/CUDA/CPU.

    Setup requis:
        1. Installer vLLM: pip install vllm
        2. Lancer le serveur: vllm serve MODEL_ID --port 8001 --device mps
           Exemple: vllm serve Qwen/Qwen2.5-3B-Instruct --port 8001 --device mps

    Args:
        model_id: ID du mod√®le HuggingFace (ex: "Qwen/Qwen2.5-3B-Instruct")
        **kwargs: Param√®tres additionnels (base_url, etc.)

    Returns:
        Instance de agno.models.vllm.VLLM configur√©e pour le serveur local
    """
    try:
        from agno.models.vllm import VLLM
    except ImportError as e:
        raise ImportError(
            "Le package Agno n'est pas correctement install√©."
        ) from e

    # Configuration par d√©faut
    base_url = kwargs.pop("base_url", "http://localhost:8001/v1/")  # Port 8001 pour vLLM
    api_key = kwargs.pop("api_key", "EMPTY")  # vLLM n'a pas besoin de cl√©

    logger.info(f"‚úÖ Creating vLLM model: {model_id}")
    logger.info(f"   Base URL: {base_url}")
    logger.info(f"   Note: Assurez-vous que le serveur vLLM est lanc√©!")
    logger.info(f"   Command: vllm serve {model_id} --port 8001 --device mps")

    return VLLM(
        id=model_id,
        name=f"vLLM {model_id.split('/')[-1]}",
        base_url=base_url,
        api_key=api_key,
        **kwargs
    )


def _create_openai_model(model_id: str, **kwargs) -> Any:
    """
    Cr√©e un mod√®le OpenAI (bonus).

    Args:
        model_id: ID du mod√®le OpenAI (ex: "gpt-4o", "gpt-4o-mini")
        **kwargs: Param√®tres additionnels (api_key, etc.)

    Returns:
        Instance de agno.models.openai.OpenAIChat
    """
    try:
        from agno.models.openai import OpenAIChat
    except ImportError as e:
        raise ImportError(
            "Le package 'openai' n'est pas install√©. "
            "Installez-le avec: uv add openai"
        ) from e

    # R√©cup√©rer la cl√© API
    api_key = kwargs.pop("api_key", None) or os.getenv("OPENAI_API_KEY")

    if not api_key:
        logger.warning(
            "‚ö†Ô∏è  OPENAI_API_KEY non configur√©e. "
            "Le mod√®le sera cr√©√© mais √©chouera √† l'ex√©cution."
        )

    logger.info(f"‚úÖ Creating OpenAI model: {model_id}")

    return OpenAIChat(
        id=model_id,
        api_key=api_key,
        **kwargs
    )


# ========================================
# Helpers pour cr√©er des mod√®les par d√©faut
# ========================================

def create_default_ollama_model(**kwargs) -> Any:
    """Cr√©e le mod√®le Ollama par d√©faut (mistral)."""
    return create_model(f"ollama:{DEFAULT_OLLAMA_MODEL}", **kwargs)


def create_default_claude_model(**kwargs) -> Any:
    """Cr√©e le mod√®le Claude par d√©faut (Sonnet 4.5)."""
    return create_model(f"anthropic:{DEFAULT_CLAUDE_MODEL}", **kwargs)


def create_default_mlx_model(**kwargs) -> Any:
    """Cr√©e le mod√®le MLX par d√©faut (Qwen 2.5 3B 4-bit)."""
    return create_model(f"mlx:{DEFAULT_MLX_MODEL}", **kwargs)


# ========================================
# Validation et tests
# ========================================

def validate_model_string(model_string: str) -> tuple[str, str]:
    """
    Valide une string de mod√®le et retourne (provider, model_id).

    Args:
        model_string: String √† valider

    Returns:
        Tuple (provider, model_id)

    Raises:
        ValueError: Si le format est invalide
    """
    if ":" not in model_string:
        raise ValueError(
            f"Format invalide: '{model_string}'. "
            f"Format attendu: 'provider:model'"
        )

    provider, model_id = model_string.split(":", 1)
    provider = provider.lower().strip()
    model_id = model_id.strip()

    valid_providers = ["ollama", "anthropic", "mlx", "vllm", "openai"]
    if provider not in valid_providers:
        raise ValueError(
            f"Provider non support√©: '{provider}'. "
            f"Providers support√©s: {', '.join(valid_providers)}"
        )

    if not model_id:
        raise ValueError("Le model_id ne peut pas √™tre vide")

    return provider, model_id


def test_model_creation():
    """Teste la cr√©ation de mod√®les."""
    print("üß™ Test de cr√©ation de mod√®les")
    print("=" * 70)

    # Test validation
    print("\n1. Test validation...")
    try:
        provider, model_id = validate_model_string("ollama:mistral")
        print(f"   ‚úÖ Validation OK: provider={provider}, model={model_id}")
    except ValueError as e:
        print(f"   ‚ùå Erreur: {e}")

    # Test cr√©ation Ollama
    print("\n2. Test cr√©ation Ollama...")
    try:
        model = create_model("ollama:mistral")
        print(f"   ‚úÖ Mod√®le cr√©√©: {model}")
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")

    # Test cr√©ation Claude
    print("\n3. Test cr√©ation Claude...")
    try:
        model = create_model("anthropic:claude-sonnet-4-5-20250929")
        print(f"   ‚úÖ Mod√®le cr√©√©: {model}")
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")

    # Test cr√©ation MLX
    print("\n4. Test cr√©ation MLX...")
    try:
        model = create_model("mlx:mlx-community/Phi-3-mini-4k-instruct-4bit")
        print(f"   ‚úÖ Mod√®le cr√©√©: {model}")
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")

    # Test cr√©ation HuggingFace
    print("\n5. Test cr√©ation HuggingFace...")
    try:
        model = create_model("huggingface:Qwen/Qwen2.5-3B-Instruct")
        print(f"   ‚úÖ Mod√®le cr√©√©: {model}")
    except Exception as e:
        print(f"   ‚ùå Erreur: {e}")

    print("\n" + "=" * 70)


if __name__ == "__main__":
    test_model_creation()
