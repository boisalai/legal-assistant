"""
Service pour g√©rer le serveur vLLM automatiquement.

Ce service d√©marre et arr√™te le serveur vLLM en fonction du mod√®le s√©lectionn√©,
permettant de charger n'importe quel mod√®le HuggingFace localement.
"""

import asyncio
import logging
import subprocess
import time
from typing import Optional

logger = logging.getLogger(__name__)


class VLLMServerService:
    """
    G√®re le lifecycle du serveur vLLM.

    Features:
    - D√©marre automatiquement le serveur vLLM avec le mod√®le demand√©
    - Arr√™te proprement le serveur en cours
    - Switch entre mod√®les (arr√™te l'ancien, d√©marre le nouveau)
    - V√©rifie la sant√© du serveur
    - Support MPS (Apple Silicon) et CUDA (NVIDIA)
    """

    def __init__(self, port: int = 8001, host: str = "localhost"):  # Port 8001 pour √©viter conflit avec FastAPI (8000)
        self.port = port
        self.host = host
        self.process: Optional[subprocess.Popen] = None
        self.current_model: Optional[str] = None
        self._startup_timeout = 60  # vLLM prend plus de temps √† d√©marrer

    def is_running(self) -> bool:
        """V√©rifie si le serveur vLLM est en cours d'ex√©cution."""
        if self.process is None:
            return False

        # V√©rifier si le processus est toujours vivant
        poll = self.process.poll()
        return poll is None

    async def health_check(self) -> bool:
        """
        V√©rifie la sant√© du serveur vLLM via un appel HTTP.

        Returns:
            True si le serveur r√©pond, False sinon
        """
        try:
            import httpx
            async with httpx.AsyncClient(timeout=5.0) as client:
                response = await client.get(f"http://{self.host}:{self.port}/v1/models")
                return response.status_code == 200
        except Exception as e:
            logger.debug(f"Health check failed: {e}")
            return False

    def _detect_device(self) -> str:
        """
        D√©tecte automatiquement le meilleur device disponible.

        Returns:
            "cuda" si NVIDIA GPU disponible, "cpu" sinon
        """
        try:
            import torch
            if torch.cuda.is_available():
                return "cuda"
        except ImportError:
            pass

        # vLLM ne supporte pas MPS officiellement
        # Sur Apple Silicon, on utilise CPU (lent) ou MLX (recommand√©)
        return "cpu"

    async def start(self, model_id: str, max_wait: int = 60, device: Optional[str] = None) -> bool:
        """
        D√©marre le serveur vLLM avec le mod√®le sp√©cifi√©.

        Args:
            model_id: ID du mod√®le HuggingFace (ex: "Qwen/Qwen2.5-3B-Instruct")
            max_wait: Temps max d'attente pour le d√©marrage (secondes)
            device: Device √† utiliser ("cuda" ou "cpu", auto-d√©tect√© si None)

        Returns:
            True si le serveur a d√©marr√© avec succ√®s, False sinon
        """
        # Si le mod√®le demand√© est d√©j√† en cours, ne rien faire
        if self.is_running() and self.current_model == model_id:
            logger.info(f"‚úÖ Serveur vLLM d√©j√† en cours avec {model_id}")
            return True

        # Si un autre mod√®le tourne, l'arr√™ter d'abord
        if self.is_running():
            logger.info(f"üîÑ Changement de mod√®le: {self.current_model} ‚Üí {model_id}")
            await self.stop()

        # D√©tecter le device si non sp√©cifi√©
        if device is None:
            device = self._detect_device()

        logger.info(f"üöÄ D√©marrage serveur vLLM avec {model_id}...")
        logger.info(f"   Port: {self.port}")
        logger.info(f"   Device: {device}")
        logger.info(f"   ‚ö†Ô∏è  Premier d√©marrage: t√©l√©chargement du mod√®le (~6-14 GB)")

        try:
            # Construire la commande vLLM
            # Note: vLLM 0.6+ d√©tecte automatiquement le device (CUDA/CPU)
            # L'argument --device n'est plus support√©
            cmd = [
                "vllm", "serve",
                model_id,
                "--port", str(self.port),
                "--host", self.host,
                "--max-model-len", "2048",  # Limite pour CPU (moins de m√©moire que GPU)
            ]

            # D√©marrer le serveur vLLM en subprocess
            self.process = subprocess.Popen(
                cmd,
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
            )

            self.current_model = model_id

            # Attendre que le serveur soit pr√™t
            logger.info(f"‚è≥ Attente du d√©marrage du serveur (max {max_wait}s)...")
            start_time = time.time()

            while time.time() - start_time < max_wait:
                if await self.health_check():
                    elapsed = time.time() - start_time
                    logger.info(f"‚úÖ Serveur vLLM d√©marr√© avec succ√®s en {elapsed:.1f}s")
                    logger.info(f"   URL: http://{self.host}:{self.port}/v1")
                    return True

                # V√©rifier si le processus a crash√©
                if not self.is_running():
                    logger.error("‚ùå Le processus vLLM s'est arr√™t√© de mani√®re inattendue")
                    if self.process and self.process.stderr:
                        stderr = self.process.stderr.read()
                        logger.error(f"Erreur: {stderr}")
                    return False

                await asyncio.sleep(2)  # vLLM prend plus de temps

            # Timeout atteint
            logger.error(f"‚ùå Timeout: Le serveur vLLM n'a pas d√©marr√© en {max_wait}s")
            await self.stop()
            return False

        except FileNotFoundError:
            logger.error("‚ùå vLLM n'est pas install√©. Installez avec: pip install vllm")
            return False
        except Exception as e:
            logger.error(f"‚ùå Erreur lors du d√©marrage du serveur vLLM: {e}")
            return False

    async def stop(self) -> None:
        """Arr√™te proprement le serveur vLLM."""
        if not self.is_running():
            logger.debug("Serveur vLLM d√©j√† arr√™t√©")
            return

        logger.info(f"üõë Arr√™t du serveur vLLM (mod√®le: {self.current_model})...")

        try:
            # Essayer d'abord SIGTERM (arr√™t propre)
            if self.process:
                self.process.terminate()

                # Attendre max 5 secondes
                for _ in range(5):
                    if self.process.poll() is not None:
                        break
                    await asyncio.sleep(1)

                # Si toujours vivant, SIGKILL
                if self.process.poll() is None:
                    logger.warning("Force kill du serveur vLLM...")
                    self.process.kill()

                self.process = None
                self.current_model = None
                logger.info("‚úÖ Serveur vLLM arr√™t√©")
        except Exception as e:
            logger.error(f"Erreur lors de l'arr√™t du serveur vLLM: {e}")

    async def restart(self, model_id: str, device: Optional[str] = None) -> bool:
        """
        Red√©marre le serveur vLLM avec un nouveau mod√®le.

        Args:
            model_id: ID du nouveau mod√®le
            device: Device √† utiliser (auto-d√©tect√© si None)

        Returns:
            True si le red√©marrage a r√©ussi
        """
        await self.stop()
        return await self.start(model_id, device=device)

    def get_status(self) -> dict:
        """
        Retourne le statut actuel du serveur vLLM.

        Returns:
            Dict avec: running, model, port, host
        """
        return {
            "running": self.is_running(),
            "model": self.current_model,
            "port": self.port,
            "host": self.host,
            "url": f"http://{self.host}:{self.port}/v1" if self.is_running() else None,
        }


# ============================================================================
# Singleton instance
# ============================================================================

_vllm_service: Optional[VLLMServerService] = None


def get_vllm_server_service() -> VLLMServerService:
    """Retourne l'instance singleton du service vLLM."""
    global _vllm_service
    if _vllm_service is None:
        _vllm_service = VLLMServerService()
    return _vllm_service


async def ensure_vllm_server(model_id: str, device: Optional[str] = None) -> bool:
    """
    Helper pour s'assurer que le serveur vLLM tourne avec le bon mod√®le.

    Args:
        model_id: ID complet du mod√®le (ex: "vllm:Qwen/..." ou "huggingface:Qwen/...")
        device: Device √† utiliser (auto-d√©tect√© si None)

    Returns:
        True si le serveur est pr√™t

    Examples:
        >>> await ensure_vllm_server("vllm:Qwen/Qwen2.5-3B-Instruct")
        True
        >>> await ensure_vllm_server("huggingface:Qwen/Qwen2.5-3B-Instruct")  # Compatibilit√©
        True
    """
    # Extraire le model_id sans le prefix "vllm:" ou "huggingface:"
    if model_id.startswith("vllm:"):
        model_id = model_id.replace("vllm:", "")
    elif model_id.startswith("huggingface:"):
        model_id = model_id.replace("huggingface:", "")

    service = get_vllm_server_service()

    # Si le bon mod√®le tourne d√©j√†, retourner imm√©diatement
    if service.is_running() and service.current_model == model_id:
        return True

    # Sinon, d√©marrer le serveur avec le mod√®le
    return await service.start(model_id, device=device)


# ============================================================================
# Cleanup au shutdown
# ============================================================================

async def shutdown_vllm_server():
    """Arr√™te le serveur vLLM au shutdown de l'application."""
    service = get_vllm_server_service()
    await service.stop()
