# Auto-d√©marrage du serveur MLX

Le backend d√©marre automatiquement le serveur MLX-LM quand vous s√©lectionnez un mod√®le MLX dans l'interface.

## Comment √ßa fonctionne

### 1. S√©lection d'un mod√®le MLX dans le chat

Quand vous s√©lectionnez un mod√®le MLX dans "Param√®tres LLM" :
- Exemple : `üçé Qwen 2.5 3B (MLX) - Recommand√© Apple Silicon`

### 2. Auto-d√©tection et d√©marrage

Le backend d√©tecte automatiquement que c'est un mod√®le MLX (prefix `mlx:`) et :

1. **V√©rifie si le serveur tourne d√©j√†**
   - Si le bon mod√®le tourne ‚Üí utilise le serveur existant
   - Si un autre mod√®le tourne ‚Üí arr√™te l'ancien, d√©marre le nouveau
   - Si aucun serveur ne tourne ‚Üí d√©marre le serveur

2. **D√©marre le serveur MLX** (si n√©cessaire)
   ```bash
   python3 -m mlx_lm.server --model <model_id> --port 8080
   ```

3. **Attend que le serveur soit pr√™t**
   - Health check toutes les secondes
   - Timeout: 30 secondes max
   - Au premier lancement: t√©l√©chargement du mod√®le (~2-4 GB)

4. **Affiche un message de statut dans le chat**
   ```
   üçé D√©marrage du serveur MLX...
   ‚úÖ Serveur MLX pr√™t
   ```

## Architecture technique

### Service `MLXServerService`

**Fichier :** `backend/services/mlx_server_service.py`

**Responsabilit√©s :**
- G√®re le lifecycle du subprocess `mlx_lm.server`
- Switch automatique entre mod√®les
- Health checks via appels HTTP
- Cleanup au shutdown de l'application

**M√©thodes principales :**
```python
service = get_mlx_server_service()

# D√©marrer avec un mod√®le
await service.start("mlx-community/Qwen2.5-3B-Instruct-4bit")

# V√©rifier le statut
status = service.get_status()  # {'running': True, 'model': '...', ...}

# Arr√™ter
await service.stop()

# Helper pour auto-start/switch
await ensure_mlx_server("mlx:mlx-community/Qwen2.5-3B-Instruct-4bit")
```

### Int√©gration dans le chat

**Fichier :** `backend/routes/chat.py`

**Logique :**
```python
async def _handle_regular_chat_stream(request: ChatRequest):
    # Auto-start MLX server if needed
    if request.model_id.startswith("mlx:"):
        await ensure_mlx_server(request.model_id)

    # Create the model
    model = create_model(request.model_id)
    ...
```

### Endpoints API

**Fichier :** `backend/routes/settings.py`

```bash
# V√©rifier le statut
GET /api/settings/mlx/status
‚Üí {"running": true, "model": "mlx-community/Qwen2.5-3B-Instruct-4bit", ...}

# D√©marrer manuellement
POST /api/settings/mlx/start
Body: {"model_id": "mlx:mlx-community/Qwen2.5-3B-Instruct-4bit"}

# Arr√™ter
POST /api/settings/mlx/stop
```

## Workflow utilisateur

### Sc√©nario 1 : Premier usage d'un mod√®le MLX

1. **Utilisateur** : S√©lectionne "üçé Qwen 2.5 3B (MLX)" dans Param√®tres LLM
2. **Utilisateur** : Envoie un message : "Bonjour"
3. **Backend** :
   - D√©tecte `mlx:mlx-community/Qwen2.5-3B-Instruct-4bit`
   - Affiche : "üçé D√©marrage du serveur MLX..."
   - T√©l√©charge le mod√®le (~2 GB, 1-2 min)
   - D√©marre le serveur MLX
   - Attend que le serveur soit pr√™t
   - Affiche : "‚úÖ Serveur MLX pr√™t"
4. **Backend** : R√©pond au message avec le mod√®le MLX

**‚è±Ô∏è Dur√©e totale (premier usage) :** ~1-3 minutes (t√©l√©chargement + d√©marrage)

### Sc√©nario 2 : R√©utilisation du m√™me mod√®le

1. **Utilisateur** : Envoie un autre message
2. **Backend** :
   - D√©tecte que le serveur MLX avec Qwen 2.5 3B tourne d√©j√†
   - Utilise directement le serveur existant
   - R√©pond imm√©diatement

**‚è±Ô∏è Dur√©e totale :** Instantan√© (pas de d√©marrage)

### Sc√©nario 3 : Switch entre mod√®les MLX

1. **Utilisateur** : Change de "Qwen 2.5 3B" ‚Üí "Llama 3.2 3B" dans Param√®tres LLM
2. **Utilisateur** : Envoie un message
3. **Backend** :
   - D√©tecte un changement de mod√®le MLX
   - Affiche : "üçé D√©marrage du serveur MLX..."
   - Arr√™te le serveur Qwen 2.5 3B
   - D√©marre le serveur Llama 3.2 3B
   - Affiche : "‚úÖ Serveur MLX pr√™t"
4. **Backend** : R√©pond avec le nouveau mod√®le

**‚è±Ô∏è Dur√©e totale :** ~10-30 secondes (si mod√®le d√©j√† t√©l√©charg√©)

### Sc√©nario 4 : Switch MLX ‚Üí Ollama

1. **Utilisateur** : Change de "MLX Qwen 2.5 3B" ‚Üí "Ollama Qwen 2.5 7B"
2. **Utilisateur** : Envoie un message
3. **Backend** :
   - D√©tecte que le mod√®le n'est pas MLX
   - Laisse le serveur MLX tourner en arri√®re-plan
   - Utilise Ollama directement

**Note :** Le serveur MLX reste actif jusqu'au shutdown du backend.

## Gestion des erreurs

### Erreur : mlx-lm non install√©

```
‚ùå √âchec du d√©marrage du serveur MLX. V√©rifiez que mlx-lm est install√© (uv sync).
```

**Solution :**
```bash
cd backend
uv sync  # mlx-lm est install√© par d√©faut
```

### Erreur : Port 8080 d√©j√† utilis√©

Le serveur MLX ne peut pas d√©marrer si le port 8080 est occup√©.

**Solution :**
```bash
# Trouver le processus
lsof -i :8080

# Tuer le processus
kill -9 <PID>
```

### Erreur : Mod√®le introuvable sur HuggingFace

Le t√©l√©chargement √©choue si le mod√®le n'existe pas.

**Solution :**
- V√©rifier l'orthographe du mod√®le
- V√©rifier la connexion Internet
- Utiliser un autre mod√®le MLX

## Optimisations

### Cache des mod√®les

Les mod√®les t√©l√©charg√©s sont cach√©s dans `~/.cache/huggingface/hub/`.

**Avantages :**
- Pas de re-t√©l√©chargement apr√®s le premier usage
- Switch rapide entre mod√®les d√©j√† t√©l√©charg√©s

**Nettoyage du cache :**
```bash
rm -rf ~/.cache/huggingface/hub/models--mlx-community*
```

### Health checks

Le service v√©rifie la sant√© du serveur toutes les secondes pendant le d√©marrage.

**Endpoint v√©rifi√© :**
```bash
GET http://localhost:8080/v1/models
```

**Timeout :** 30 secondes max

## Shutdown propre

Au shutdown du backend (`Ctrl+C`), le serveur MLX est arr√™t√© automatiquement.

**Logs :**
```
Legal Assistant API - Shutting down...
MLX server stopped
SurrealDB disconnected
Goodbye!
```

## Troubleshooting

### Le serveur ne d√©marre pas

**1. V√©rifier les logs du backend**
```bash
cd backend
uv run python main.py
# Observer les logs lors de la s√©lection d'un mod√®le MLX
```

**2. Tester manuellement le d√©marrage**
```bash
python3 -m mlx_lm.server --model mlx-community/Qwen2.5-3B-Instruct-4bit --port 8080
```

**3. V√©rifier l'installation de mlx-lm**
```bash
python3 -c "import mlx_lm; print(mlx_lm.__version__)"
```

### Le serveur d√©marre mais ne r√©pond pas

**V√©rifier le health check :**
```bash
curl http://localhost:8080/v1/models
```

**R√©sultat attendu :**
```json
{"data": [...], "object": "list"}
```

## Comparaison : Auto vs Manuel

| Crit√®re | Auto-start | Manuel |
|---------|------------|--------|
| **Setup utilisateur** | Aucun | Terminal s√©par√© |
| **Switch mod√®les** | Automatique | Red√©marrer manuellement |
| **Gestion processus** | Backend | Utilisateur |
| **Cleanup** | Automatique | Manuel (Ctrl+C) |
| **Premier d√©marrage** | ~1-3 min | ~1-3 min |
| **Complexit√©** | Simple | Technique |

**Verdict :** L'auto-start simplifie grandement l'exp√©rience utilisateur.

## R√©f√©rences

- **Service source :** `backend/services/mlx_server_service.py`
- **Int√©gration chat :** `backend/routes/chat.py:590-612`
- **Endpoints API :** `backend/routes/settings.py:165-238`
- **Guide utilisateur :** `backend/MLX_GUIDE.md`
