# Configuration Ollama pour Tests Locaux

> Guide pour installer et tester le workflow avec Ollama
> Date: 2025-11-19

## üìñ Vue d'ensemble

Ollama permet d'ex√©cuter des mod√®les LLM localement sans API externe.

**Avantages:**
- ‚úÖ Gratuit et illimit√©
- ‚úÖ Fonctionne offline
- ‚úÖ Pas besoin de cl√© API
- ‚úÖ Parfait pour tests CI/CD
- ‚úÖ Supporte plusieurs mod√®les (Mistral, Llama2, Phi, etc.)

**Utilisations dans le projet:**
- Tests automatis√©s (CI/CD)
- D√©veloppement local sans co√ªts
- Validation rapide des workflows
- Alternative √† Claude API pour prototypage

---

## üöÄ Installation

### macOS

```bash
# Via Homebrew
brew install ollama

# Ou t√©l√©charger depuis le site
# https://ollama.com/download
```

### Linux

```bash
# Script officiel
curl -fsSL https://ollama.com/install.sh | sh

# Ou installation manuelle
curl -L https://github.com/ollama/ollama/releases/latest/download/ollama-linux-amd64 -o /usr/local/bin/ollama
chmod +x /usr/local/bin/ollama
```

### Windows

T√©l√©charger depuis: https://ollama.com/download

---

## üîß Configuration

### 1. D√©marrer Ollama

```bash
# Lancer le serveur (√©coute sur localhost:11434)
ollama serve

# Laisser tourner en arri√®re-plan
# Le serveur doit rester actif pendant les tests
```

### 2. T√©l√©charger un mod√®le

```bash
# Mistral (recommand√© - ~4GB)
ollama pull mistral

# Llama2 (alternative - ~4GB)
ollama pull llama2

# Phi-3 (plus l√©ger - ~2GB)
ollama pull phi

# V√©rifier les mod√®les install√©s
ollama list
```

### 3. Tester le mod√®le

```bash
# Test interactif
ollama run mistral "Bonjour, comment √ßa va?"

# Devrait r√©pondre en fran√ßais
```

---

## üß™ Tests avec le Workflow Notary

### Pr√©requis

1. **SurrealDB lanc√©:**
   ```bash
   cd /path/to/notary
   docker-compose up -d surrealdb
   ```

2. **Ollama lanc√©:**
   ```bash
   ollama serve
   # Dans un terminal s√©par√©
   ```

3. **Mod√®le t√©l√©charg√©:**
   ```bash
   ollama pull mistral
   ```

### Lancer le test

```bash
cd backend

# Test avec Mistral (d√©faut)
uv run python test_workflow_ollama.py

# Test avec un autre mod√®le
MODEL=ollama:llama2 uv run python test_workflow_ollama.py
MODEL=ollama:phi uv run python test_workflow_ollama.py

# Pour comparaison avec Claude
export ANTHROPIC_API_KEY=your_key
MODEL=anthropic:claude-sonnet-4-5-20250929 uv run python test_workflow_ollama.py
```

### Sortie attendue

```
üß™ TEST WORKFLOW AGNO + OLLAMA
======================================================================
üß™ Test du workflow avec mod√®le: ollama:mistral
üìä SurrealDB URL: ws://localhost:8000
‚úÖ AgnoDBService initialized
‚úÖ Workflow created with automatic persistence
üìÑ 3 fichier(s) PDF √† analyser
üöÄ Lancement du workflow...

[... logs d'ex√©cution ...]

‚úÖ Workflow termin√© en 45.23s
======================================================================
üìä R√âSULTATS DU WORKFLOW
======================================================================
‚úÖ Succ√®s!
üìã Score de confiance: 78.00%
‚ö†Ô∏è  Requiert validation: True

üìù Checklist g√©n√©r√©e:
   - Items: 12
   - Points d'attention: 3
   - Documents manquants: 2

üîç V√©rification de la persistance dans SurrealDB...
‚úÖ 1 workflow run(s) trouv√©(s)
   Run #1:
      - ID: workflow_runs:abc123
      - Created: 2025-11-19T10:30:00Z
      - Status: completed

======================================================================
‚úÖ TEST R√âUSSI
======================================================================
```

---

## üìä Mod√®les Recommand√©s

| Mod√®le | Taille | Vitesse | Qualit√© | Usage |
|--------|--------|---------|---------|-------|
| **mistral** | ~4GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Production locale, fran√ßais |
| **llama2** | ~4GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Alternative stable |
| **phi** | ~2GB | ‚ö°‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê | Tests rapides, CI/CD |
| **codellama** | ~4GB | ‚ö°‚ö°‚ö° | ‚≠ê‚≠ê‚≠ê‚≠ê | Code, d√©veloppement |

### S√©lectionner un mod√®le

Crit√®res:
1. **Fran√ßais requis:** Mistral ou Llama2
2. **Vitesse maximale:** Phi
3. **Meilleure qualit√©:** Mistral
4. **Espace limit√©:** Phi

---

## üîç V√©rification de la Persistance

### Requ√™te manuelle SurrealDB

```bash
# Voir tous les workflow runs
curl -X POST http://localhost:8001/sql \
  -H "Accept: application/json" \
  -H "NS: agno" \
  -H "DB: notary_db" \
  -u "root:root" \
  -d "SELECT * FROM workflow_runs ORDER BY created_at DESC LIMIT 5;"

# Workflow runs pour un dossier sp√©cifique
curl -X POST http://localhost:8001/sql \
  -H "Accept: application/json" \
  -H "NS: agno" \
  -H "DB: notary_db" \
  -u "root:root" \
  -d "SELECT * FROM workflow_runs WHERE metadata.dossier_id = 'dossier:test_ollama_20251119';"
```

### Via Python

```python
from services.agno_db_service import get_agno_db_service

# R√©cup√©rer l'historique
service = get_agno_db_service()
history = await service.get_workflow_history(
    dossier_id="dossier:test_ollama_20251119",
    limit=10
)

for run in history:
    print(f"Run: {run['id']}, Status: {run.get('status')}")
```

---

## üêõ Troubleshooting

### Erreur: "connection refused"

**Probl√®me:** Ollama n'est pas lanc√©
**Solution:**
```bash
# V√©rifier si Ollama tourne
curl http://localhost:11434/api/version

# Si erreur, lancer:
ollama serve
```

### Erreur: "model not found"

**Probl√®me:** Mod√®le pas t√©l√©charg√©
**Solution:**
```bash
# T√©l√©charger le mod√®le
ollama pull mistral

# V√©rifier
ollama list
```

### Workflow tr√®s lent

**Cause possible:** CPU seulement (pas de GPU)
**Solutions:**
1. Utiliser un mod√®le plus petit (phi au lieu de mistral)
2. R√©duire le nombre de PDFs √† analyser
3. Utiliser Claude API pour production

### R√©ponses incoh√©rentes

**Cause:** Mod√®le trop petit ou mal adapt√©
**Solutions:**
1. Utiliser Mistral (meilleur fran√ßais)
2. Ajuster les prompts dans le workflow
3. Utiliser Claude API pour qualit√© maximale

---

## üìà Comparaison des Providers

| Provider | Co√ªt | Vitesse | Qualit√© | Offline | Usage |
|----------|------|---------|---------|---------|-------|
| **Ollama** | Gratuit | Moyen | Bon | ‚úÖ | Dev, tests |
| **Claude API** | Payant | Rapide | Excellent | ‚ùå | Production |
| **MLX (Mac)** | Gratuit | Tr√®s rapide | Bon | ‚úÖ | Dev Mac |

### Strat√©gie Recommand√©e

1. **D√©veloppement:** Ollama (gratuit, illimit√©)
2. **Tests CI/CD:** Ollama (automatisable)
3. **Production:** Claude API (qualit√© maximale)
4. **Mac local:** MLX (ultra-rapide sur M1/M2)

---

## üîó Ressources

- Site officiel: https://ollama.com
- Documentation: https://github.com/ollama/ollama
- Mod√®les disponibles: https://ollama.com/library
- Agno + Ollama: https://docs.agno.com/concepts/models/ollama

---

**Maintenu par:** Claude Code
**Derni√®re mise √† jour:** 2025-11-19
**Sprint:** 1 (Foundation)
